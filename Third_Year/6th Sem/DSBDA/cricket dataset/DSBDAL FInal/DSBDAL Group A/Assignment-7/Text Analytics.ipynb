{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7 - Text Analytics\n",
    "\n",
    "##### Kaustubh Shrikant Kabra\n",
    "##### ERP Number :- 38\n",
    "##### TE Comp 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNZ-dyrBSqhD"
   },
   "source": [
    " **Text Analytics**\n",
    " 1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. \n",
    " 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650045710786,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "p_M1NEg1SsRe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650045710787,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "OzT8RooLS8lH"
   },
   "outputs": [],
   "source": [
    "text = '''It was a Thursday, but it felt like a Monday to John. And John loved Mondays. He thrived at work. He dismissed the old cliché of dreading Monday mornings and refused to engage in water-cooler complaints about “the grind” and empty conversations that included the familiar parry “How was your weekend?” “Too short!”. Yes, John liked his work and was unashamed.\n",
    "\n",
    "I should probably get another latte. I’ve just been sitting here with this empty cup. But then I’ll start to get jittery. I’ll get a decaf. No, that’s stupid, it feels stupid to pay for a decaf. I can’t justify that.\n",
    "\n",
    "John was always impatient on the weekends; he missed the formal structure of the business week. When he was younger he used to stay late after school on Fridays and come in early on Mondays, a pattern his mother referred to with equal parts admiration and disdain as “studying overtime.”\n",
    "\n",
    "Jesus, I’ve written another loser. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB8zapCNxXyE"
   },
   "source": [
    "### Tokenization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650045710788,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "MCFZ-h27vyd8"
   },
   "outputs": [],
   "source": [
    "text_split = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2651,
     "status": "ok",
     "timestamp": 1650045713430,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "5DoBkpwcw7IX",
    "outputId": "0dc7bda3-7a48-4a3b-cc00-4e8d4218db93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650045713431,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "eBcXxicC0ldd"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1650045713432,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "v2KqHOtn7c-h"
   },
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1650045713432,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "5PLGqr9NxDPh",
    "outputId": "c3212d00-ba17-41ba-8690-1f786a570a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), ('Thursday', 'NNP'), (',', ','), ('felt', 'VBD'), ('like', 'IN'), ('Monday', 'NNP'), ('John', 'NNP'), ('.', '.')]\n",
      "[('And', 'CC'), ('John', 'NNP'), ('loved', 'VBD'), ('Mondays', 'NNP'), ('.', '.')]\n",
      "[('He', 'PRP'), ('thrived', 'VBD'), ('work', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('dismissed', 'VBD'), ('old', 'JJ'), ('cliché', 'NN'), ('dreading', 'VBG'), ('Monday', 'NNP'), ('mornings', 'NNS'), ('refused', 'VBD'), ('engage', 'JJ'), ('water-cooler', 'JJ'), ('complaints', 'NNS'), ('“', 'JJ'), ('grind', 'VBP'), ('”', 'JJ'), ('empty', 'JJ'), ('conversations', 'NNS'), ('included', 'VBD'), ('familiar', 'JJ'), ('parry', 'NN'), ('“', 'NNP'), ('How', 'NNP'), ('weekend', 'NN'), ('?', '.'), ('”', 'JJ'), ('“', 'NNP'), ('Too', 'NNP'), ('short', 'JJ'), ('!', '.'), ('”', 'NN'), ('.', '.')]\n",
      "[('Yes', 'UH'), (',', ','), ('John', 'NNP'), ('liked', 'VBD'), ('work', 'NN'), ('unashamed', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('probably', 'RB'), ('get', 'VB'), ('another', 'DT'), ('latte', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('’', 'VBP'), ('sitting', 'VBG'), ('empty', 'JJ'), ('cup', 'NN'), ('.', '.')]\n",
      "[('But', 'CC'), ('I', 'PRP'), ('’', 'VBP'), ('start', 'JJ'), ('get', 'VB'), ('jittery', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('’', 'VBP'), ('get', 'VB'), ('decaf', 'NN'), ('.', '.')]\n",
      "[('No', 'DT'), (',', ','), ('’', 'FW'), ('stupid', 'JJ'), (',', ','), ('feels', 'JJ'), ('stupid', 'JJ'), ('pay', 'NN'), ('decaf', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('’', 'VBP'), ('justify', 'NN'), ('.', '.')]\n",
      "[('John', 'NNP'), ('always', 'RB'), ('impatient', 'JJ'), ('weekends', 'NNS'), (';', ':'), ('missed', 'VBN'), ('formal', 'JJ'), ('structure', 'NN'), ('business', 'NN'), ('week', 'NN'), ('.', '.')]\n",
      "[('When', 'WRB'), ('younger', 'JJR'), ('used', 'VBD'), ('stay', 'NN'), ('late', 'JJ'), ('school', 'NN'), ('Fridays', 'NNP'), ('come', 'VBP'), ('early', 'JJ'), ('Mondays', 'NNP'), (',', ','), ('pattern', 'NN'), ('mother', 'NN'), ('referred', 'VBD'), ('equal', 'JJ'), ('parts', 'NNS'), ('admiration', 'NN'), ('disdain', 'VBP'), ('“', 'JJ'), ('studying', 'VBG'), ('overtime.', 'JJ'), ('”', 'NNP'), ('Jesus', 'NNP'), (',', ','), ('I', 'PRP'), ('’', 'VBP'), ('written', 'VBN'), ('another', 'DT'), ('loser', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokenized = sent_tokenize(text)\n",
    "for i in tokenized:\n",
    "     \n",
    "    wordsList = nltk.word_tokenize(i)\n",
    " \n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    " \n",
    "    # Using a Tagger. Which is part-of-speech\n",
    "    # tagger or POS-tagger.\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    " \n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1650045713433,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "bpm3Pnrz1GVK",
    "outputId": "d5948dcd-1a02-4b9b-f80e-9b3585fd4af0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a Thursday, but it felt like a Monday to John.',\n",
       " 'And John loved Mondays.',\n",
       " 'He thrived at work.',\n",
       " 'He dismissed the old cliché of dreading Monday mornings and refused to engage in water-cooler complaints about “the grind” and empty conversations that included the familiar parry “How was your weekend?” “Too short!”.',\n",
       " 'Yes, John liked his work and was unashamed.',\n",
       " 'I should probably get another latte.',\n",
       " 'I’ve just been sitting here with this empty cup.',\n",
       " 'But then I’ll start to get jittery.',\n",
       " 'I’ll get a decaf.',\n",
       " 'No, that’s stupid, it feels stupid to pay for a decaf.',\n",
       " 'I can’t justify that.',\n",
       " 'John was always impatient on the weekends; he missed the formal structure of the business week.',\n",
       " 'When he was younger he used to stay late after school on Fridays and come in early on Mondays, a pattern his mother referred to with equal parts admiration and disdain as “studying overtime.”\\n\\nJesus, I’ve written another loser.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnsfCAsg4nLK"
   },
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1650045713434,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "yOHIbwZN15MU"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1650045713435,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "kyRJ-ZjF47Hd"
   },
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1650045713436,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "0Rul0mut4-CB"
   },
   "outputs": [],
   "source": [
    "nltk_token = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1650045713437,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "LWDqhIIB5PUr",
    "outputId": "89818d76-6337-44f4-b756-db9fa8e430cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual : It , Stem: it\n",
      "Actual : was , Stem: wa\n",
      "Actual : a , Stem: a\n",
      "Actual : Thursday , Stem: thursday\n",
      "Actual : , , Stem: ,\n",
      "Actual : but , Stem: but\n",
      "Actual : it , Stem: it\n",
      "Actual : felt , Stem: felt\n",
      "Actual : like , Stem: like\n",
      "Actual : a , Stem: a\n",
      "Actual : Monday , Stem: monday\n",
      "Actual : to , Stem: to\n",
      "Actual : John , Stem: john\n",
      "Actual : . , Stem: .\n",
      "Actual : And , Stem: and\n",
      "Actual : John , Stem: john\n",
      "Actual : loved , Stem: love\n",
      "Actual : Mondays , Stem: monday\n",
      "Actual : . , Stem: .\n",
      "Actual : He , Stem: he\n",
      "Actual : thrived , Stem: thrive\n",
      "Actual : at , Stem: at\n",
      "Actual : work , Stem: work\n",
      "Actual : . , Stem: .\n",
      "Actual : He , Stem: he\n",
      "Actual : dismissed , Stem: dismiss\n",
      "Actual : the , Stem: the\n",
      "Actual : old , Stem: old\n",
      "Actual : cliché , Stem: cliché\n",
      "Actual : of , Stem: of\n",
      "Actual : dreading , Stem: dread\n",
      "Actual : Monday , Stem: monday\n",
      "Actual : mornings , Stem: morn\n",
      "Actual : and , Stem: and\n",
      "Actual : refused , Stem: refus\n",
      "Actual : to , Stem: to\n",
      "Actual : engage , Stem: engag\n",
      "Actual : in , Stem: in\n",
      "Actual : water-cooler , Stem: water-cool\n",
      "Actual : complaints , Stem: complaint\n",
      "Actual : about , Stem: about\n",
      "Actual : “ , Stem: “\n",
      "Actual : the , Stem: the\n",
      "Actual : grind , Stem: grind\n",
      "Actual : ” , Stem: ”\n",
      "Actual : and , Stem: and\n",
      "Actual : empty , Stem: empti\n",
      "Actual : conversations , Stem: convers\n",
      "Actual : that , Stem: that\n",
      "Actual : included , Stem: includ\n",
      "Actual : the , Stem: the\n",
      "Actual : familiar , Stem: familiar\n",
      "Actual : parry , Stem: parri\n",
      "Actual : “ , Stem: “\n",
      "Actual : How , Stem: how\n",
      "Actual : was , Stem: wa\n",
      "Actual : your , Stem: your\n",
      "Actual : weekend , Stem: weekend\n",
      "Actual : ? , Stem: ?\n",
      "Actual : ” , Stem: ”\n",
      "Actual : “ , Stem: “\n",
      "Actual : Too , Stem: too\n",
      "Actual : short , Stem: short\n",
      "Actual : ! , Stem: !\n",
      "Actual : ” , Stem: ”\n",
      "Actual : . , Stem: .\n",
      "Actual : Yes , Stem: ye\n",
      "Actual : , , Stem: ,\n",
      "Actual : John , Stem: john\n",
      "Actual : liked , Stem: like\n",
      "Actual : his , Stem: hi\n",
      "Actual : work , Stem: work\n",
      "Actual : and , Stem: and\n",
      "Actual : was , Stem: wa\n",
      "Actual : unashamed , Stem: unasham\n",
      "Actual : . , Stem: .\n",
      "Actual : I , Stem: i\n",
      "Actual : should , Stem: should\n",
      "Actual : probably , Stem: probabl\n",
      "Actual : get , Stem: get\n",
      "Actual : another , Stem: anoth\n",
      "Actual : latte , Stem: latt\n",
      "Actual : . , Stem: .\n",
      "Actual : I , Stem: i\n",
      "Actual : ’ , Stem: ’\n",
      "Actual : ve , Stem: ve\n",
      "Actual : just , Stem: just\n",
      "Actual : been , Stem: been\n",
      "Actual : sitting , Stem: sit\n",
      "Actual : here , Stem: here\n",
      "Actual : with , Stem: with\n",
      "Actual : this , Stem: thi\n",
      "Actual : empty , Stem: empti\n",
      "Actual : cup , Stem: cup\n",
      "Actual : . , Stem: .\n",
      "Actual : But , Stem: but\n",
      "Actual : then , Stem: then\n",
      "Actual : I , Stem: i\n",
      "Actual : ’ , Stem: ’\n",
      "Actual : ll , Stem: ll\n",
      "Actual : start , Stem: start\n",
      "Actual : to , Stem: to\n",
      "Actual : get , Stem: get\n",
      "Actual : jittery , Stem: jitteri\n",
      "Actual : . , Stem: .\n",
      "Actual : I , Stem: i\n",
      "Actual : ’ , Stem: ’\n",
      "Actual : ll , Stem: ll\n",
      "Actual : get , Stem: get\n",
      "Actual : a , Stem: a\n",
      "Actual : decaf , Stem: decaf\n",
      "Actual : . , Stem: .\n",
      "Actual : No , Stem: no\n",
      "Actual : , , Stem: ,\n",
      "Actual : that , Stem: that\n",
      "Actual : ’ , Stem: ’\n",
      "Actual : s , Stem: s\n",
      "Actual : stupid , Stem: stupid\n",
      "Actual : , , Stem: ,\n",
      "Actual : it , Stem: it\n",
      "Actual : feels , Stem: feel\n",
      "Actual : stupid , Stem: stupid\n",
      "Actual : to , Stem: to\n",
      "Actual : pay , Stem: pay\n",
      "Actual : for , Stem: for\n",
      "Actual : a , Stem: a\n",
      "Actual : decaf , Stem: decaf\n",
      "Actual : . , Stem: .\n",
      "Actual : I , Stem: i\n",
      "Actual : can , Stem: can\n",
      "Actual : ’ , Stem: ’\n",
      "Actual : t , Stem: t\n",
      "Actual : justify , Stem: justifi\n",
      "Actual : that , Stem: that\n",
      "Actual : . , Stem: .\n",
      "Actual : John , Stem: john\n",
      "Actual : was , Stem: wa\n",
      "Actual : always , Stem: alway\n",
      "Actual : impatient , Stem: impati\n",
      "Actual : on , Stem: on\n",
      "Actual : the , Stem: the\n",
      "Actual : weekends , Stem: weekend\n",
      "Actual : ; , Stem: ;\n",
      "Actual : he , Stem: he\n",
      "Actual : missed , Stem: miss\n",
      "Actual : the , Stem: the\n",
      "Actual : formal , Stem: formal\n",
      "Actual : structure , Stem: structur\n",
      "Actual : of , Stem: of\n",
      "Actual : the , Stem: the\n",
      "Actual : business , Stem: busi\n",
      "Actual : week , Stem: week\n",
      "Actual : . , Stem: .\n",
      "Actual : When , Stem: when\n",
      "Actual : he , Stem: he\n",
      "Actual : was , Stem: wa\n",
      "Actual : younger , Stem: younger\n",
      "Actual : he , Stem: he\n",
      "Actual : used , Stem: use\n",
      "Actual : to , Stem: to\n",
      "Actual : stay , Stem: stay\n",
      "Actual : late , Stem: late\n",
      "Actual : after , Stem: after\n",
      "Actual : school , Stem: school\n",
      "Actual : on , Stem: on\n",
      "Actual : Fridays , Stem: friday\n",
      "Actual : and , Stem: and\n",
      "Actual : come , Stem: come\n",
      "Actual : in , Stem: in\n",
      "Actual : early , Stem: earli\n",
      "Actual : on , Stem: on\n",
      "Actual : Mondays , Stem: monday\n",
      "Actual : , , Stem: ,\n",
      "Actual : a , Stem: a\n",
      "Actual : pattern , Stem: pattern\n",
      "Actual : his , Stem: hi\n",
      "Actual : mother , Stem: mother\n",
      "Actual : referred , Stem: refer\n",
      "Actual : to , Stem: to\n",
      "Actual : with , Stem: with\n",
      "Actual : equal , Stem: equal\n",
      "Actual : parts , Stem: part\n",
      "Actual : admiration , Stem: admir\n",
      "Actual : and , Stem: and\n",
      "Actual : disdain , Stem: disdain\n",
      "Actual : as , Stem: as\n",
      "Actual : “ , Stem: “\n",
      "Actual : studying , Stem: studi\n",
      "Actual : overtime. , Stem: overtime.\n",
      "Actual : ” , Stem: ”\n",
      "Actual : Jesus , Stem: jesu\n",
      "Actual : , , Stem: ,\n",
      "Actual : I , Stem: i\n",
      "Actual : ’ , Stem: ’\n",
      "Actual : ve , Stem: ve\n",
      "Actual : written , Stem: written\n",
      "Actual : another , Stem: anoth\n",
      "Actual : loser , Stem: loser\n",
      "Actual : . , Stem: .\n"
     ]
    }
   ],
   "source": [
    "for w in nltk_token:\n",
    "  print(\"Actual : %s , Stem: %s\" %(w, porter_stemmer.stem(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJMVzevO6j_f"
   },
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1650045713437,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "i8eib-7Q57dj"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1625,
     "status": "ok",
     "timestamp": 1650045715042,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "USJHCwfL7RWY",
    "outputId": "d2e51e84-76d7-4666-913a-3e0104505ee5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ORIONORIGINAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1364,
     "status": "ok",
     "timestamp": 1650045716400,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "atAkjtJ46uQZ",
    "outputId": "d03e4667-6672-449f-c51d-cea8ee4894ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual : It , Lemme: It\n",
      "Actual : was , Lemme: wa\n",
      "Actual : a , Lemme: a\n",
      "Actual : Thursday , Lemme: Thursday\n",
      "Actual : , , Lemme: ,\n",
      "Actual : but , Lemme: but\n",
      "Actual : it , Lemme: it\n",
      "Actual : felt , Lemme: felt\n",
      "Actual : like , Lemme: like\n",
      "Actual : a , Lemme: a\n",
      "Actual : Monday , Lemme: Monday\n",
      "Actual : to , Lemme: to\n",
      "Actual : John , Lemme: John\n",
      "Actual : . , Lemme: .\n",
      "Actual : And , Lemme: And\n",
      "Actual : John , Lemme: John\n",
      "Actual : loved , Lemme: loved\n",
      "Actual : Mondays , Lemme: Mondays\n",
      "Actual : . , Lemme: .\n",
      "Actual : He , Lemme: He\n",
      "Actual : thrived , Lemme: thrived\n",
      "Actual : at , Lemme: at\n",
      "Actual : work , Lemme: work\n",
      "Actual : . , Lemme: .\n",
      "Actual : He , Lemme: He\n",
      "Actual : dismissed , Lemme: dismissed\n",
      "Actual : the , Lemme: the\n",
      "Actual : old , Lemme: old\n",
      "Actual : cliché , Lemme: cliché\n",
      "Actual : of , Lemme: of\n",
      "Actual : dreading , Lemme: dreading\n",
      "Actual : Monday , Lemme: Monday\n",
      "Actual : mornings , Lemme: morning\n",
      "Actual : and , Lemme: and\n",
      "Actual : refused , Lemme: refused\n",
      "Actual : to , Lemme: to\n",
      "Actual : engage , Lemme: engage\n",
      "Actual : in , Lemme: in\n",
      "Actual : water-cooler , Lemme: water-cooler\n",
      "Actual : complaints , Lemme: complaint\n",
      "Actual : about , Lemme: about\n",
      "Actual : “ , Lemme: “\n",
      "Actual : the , Lemme: the\n",
      "Actual : grind , Lemme: grind\n",
      "Actual : ” , Lemme: ”\n",
      "Actual : and , Lemme: and\n",
      "Actual : empty , Lemme: empty\n",
      "Actual : conversations , Lemme: conversation\n",
      "Actual : that , Lemme: that\n",
      "Actual : included , Lemme: included\n",
      "Actual : the , Lemme: the\n",
      "Actual : familiar , Lemme: familiar\n",
      "Actual : parry , Lemme: parry\n",
      "Actual : “ , Lemme: “\n",
      "Actual : How , Lemme: How\n",
      "Actual : was , Lemme: wa\n",
      "Actual : your , Lemme: your\n",
      "Actual : weekend , Lemme: weekend\n",
      "Actual : ? , Lemme: ?\n",
      "Actual : ” , Lemme: ”\n",
      "Actual : “ , Lemme: “\n",
      "Actual : Too , Lemme: Too\n",
      "Actual : short , Lemme: short\n",
      "Actual : ! , Lemme: !\n",
      "Actual : ” , Lemme: ”\n",
      "Actual : . , Lemme: .\n",
      "Actual : Yes , Lemme: Yes\n",
      "Actual : , , Lemme: ,\n",
      "Actual : John , Lemme: John\n",
      "Actual : liked , Lemme: liked\n",
      "Actual : his , Lemme: his\n",
      "Actual : work , Lemme: work\n",
      "Actual : and , Lemme: and\n",
      "Actual : was , Lemme: wa\n",
      "Actual : unashamed , Lemme: unashamed\n",
      "Actual : . , Lemme: .\n",
      "Actual : I , Lemme: I\n",
      "Actual : should , Lemme: should\n",
      "Actual : probably , Lemme: probably\n",
      "Actual : get , Lemme: get\n",
      "Actual : another , Lemme: another\n",
      "Actual : latte , Lemme: latte\n",
      "Actual : . , Lemme: .\n",
      "Actual : I , Lemme: I\n",
      "Actual : ’ , Lemme: ’\n",
      "Actual : ve , Lemme: ve\n",
      "Actual : just , Lemme: just\n",
      "Actual : been , Lemme: been\n",
      "Actual : sitting , Lemme: sitting\n",
      "Actual : here , Lemme: here\n",
      "Actual : with , Lemme: with\n",
      "Actual : this , Lemme: this\n",
      "Actual : empty , Lemme: empty\n",
      "Actual : cup , Lemme: cup\n",
      "Actual : . , Lemme: .\n",
      "Actual : But , Lemme: But\n",
      "Actual : then , Lemme: then\n",
      "Actual : I , Lemme: I\n",
      "Actual : ’ , Lemme: ’\n",
      "Actual : ll , Lemme: ll\n",
      "Actual : start , Lemme: start\n",
      "Actual : to , Lemme: to\n",
      "Actual : get , Lemme: get\n",
      "Actual : jittery , Lemme: jittery\n",
      "Actual : . , Lemme: .\n",
      "Actual : I , Lemme: I\n",
      "Actual : ’ , Lemme: ’\n",
      "Actual : ll , Lemme: ll\n",
      "Actual : get , Lemme: get\n",
      "Actual : a , Lemme: a\n",
      "Actual : decaf , Lemme: decaf\n",
      "Actual : . , Lemme: .\n",
      "Actual : No , Lemme: No\n",
      "Actual : , , Lemme: ,\n",
      "Actual : that , Lemme: that\n",
      "Actual : ’ , Lemme: ’\n",
      "Actual : s , Lemme: s\n",
      "Actual : stupid , Lemme: stupid\n",
      "Actual : , , Lemme: ,\n",
      "Actual : it , Lemme: it\n",
      "Actual : feels , Lemme: feel\n",
      "Actual : stupid , Lemme: stupid\n",
      "Actual : to , Lemme: to\n",
      "Actual : pay , Lemme: pay\n",
      "Actual : for , Lemme: for\n",
      "Actual : a , Lemme: a\n",
      "Actual : decaf , Lemme: decaf\n",
      "Actual : . , Lemme: .\n",
      "Actual : I , Lemme: I\n",
      "Actual : can , Lemme: can\n",
      "Actual : ’ , Lemme: ’\n",
      "Actual : t , Lemme: t\n",
      "Actual : justify , Lemme: justify\n",
      "Actual : that , Lemme: that\n",
      "Actual : . , Lemme: .\n",
      "Actual : John , Lemme: John\n",
      "Actual : was , Lemme: wa\n",
      "Actual : always , Lemme: always\n",
      "Actual : impatient , Lemme: impatient\n",
      "Actual : on , Lemme: on\n",
      "Actual : the , Lemme: the\n",
      "Actual : weekends , Lemme: weekend\n",
      "Actual : ; , Lemme: ;\n",
      "Actual : he , Lemme: he\n",
      "Actual : missed , Lemme: missed\n",
      "Actual : the , Lemme: the\n",
      "Actual : formal , Lemme: formal\n",
      "Actual : structure , Lemme: structure\n",
      "Actual : of , Lemme: of\n",
      "Actual : the , Lemme: the\n",
      "Actual : business , Lemme: business\n",
      "Actual : week , Lemme: week\n",
      "Actual : . , Lemme: .\n",
      "Actual : When , Lemme: When\n",
      "Actual : he , Lemme: he\n",
      "Actual : was , Lemme: wa\n",
      "Actual : younger , Lemme: younger\n",
      "Actual : he , Lemme: he\n",
      "Actual : used , Lemme: used\n",
      "Actual : to , Lemme: to\n",
      "Actual : stay , Lemme: stay\n",
      "Actual : late , Lemme: late\n",
      "Actual : after , Lemme: after\n",
      "Actual : school , Lemme: school\n",
      "Actual : on , Lemme: on\n",
      "Actual : Fridays , Lemme: Fridays\n",
      "Actual : and , Lemme: and\n",
      "Actual : come , Lemme: come\n",
      "Actual : in , Lemme: in\n",
      "Actual : early , Lemme: early\n",
      "Actual : on , Lemme: on\n",
      "Actual : Mondays , Lemme: Mondays\n",
      "Actual : , , Lemme: ,\n",
      "Actual : a , Lemme: a\n",
      "Actual : pattern , Lemme: pattern\n",
      "Actual : his , Lemme: his\n",
      "Actual : mother , Lemme: mother\n",
      "Actual : referred , Lemme: referred\n",
      "Actual : to , Lemme: to\n",
      "Actual : with , Lemme: with\n",
      "Actual : equal , Lemme: equal\n",
      "Actual : parts , Lemme: part\n",
      "Actual : admiration , Lemme: admiration\n",
      "Actual : and , Lemme: and\n",
      "Actual : disdain , Lemme: disdain\n",
      "Actual : as , Lemme: a\n",
      "Actual : “ , Lemme: “\n",
      "Actual : studying , Lemme: studying\n",
      "Actual : overtime. , Lemme: overtime.\n",
      "Actual : ” , Lemme: ”\n",
      "Actual : Jesus , Lemme: Jesus\n",
      "Actual : , , Lemme: ,\n",
      "Actual : I , Lemme: I\n",
      "Actual : ’ , Lemme: ’\n",
      "Actual : ve , Lemme: ve\n",
      "Actual : written , Lemme: written\n",
      "Actual : another , Lemme: another\n",
      "Actual : loser , Lemme: loser\n",
      "Actual : . , Lemme: .\n"
     ]
    }
   ],
   "source": [
    "for w in nltk_token:\n",
    "  print(\"Actual : %s , Lemme: %s\" %(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssmuE63J7iQP"
   },
   "source": [
    "## 2. Word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaPiEmrb9pLJ"
   },
   "source": [
    "### Term Frequency **(TF)**\n",
    "### Formula:     **tf(t,d)** = count of t in d / number of words in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1650045716401,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "y2owY1-17IF4"
   },
   "outputs": [],
   "source": [
    "sentence1 = \"Data Science is the best job of the 21st century\"\n",
    "sentence2 = \"machine learning is the key for data science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1650045716402,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "DknjAyow_b5i"
   },
   "outputs": [],
   "source": [
    "# Spliting both sentences\n",
    "sentence1 = sentence1.split(\" \")\n",
    "sentence2 = sentence2.split(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1650045716403,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "Viz6rT2v_7Nd"
   },
   "outputs": [],
   "source": [
    "join = set(sentence1).union(set(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1650045716403,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "xhk2_RzmADWf",
    "outputId": "4880aee5-10af-4423-ec4b-dff3f6c0ec49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'21st',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'best',\n",
       " 'century',\n",
       " 'data',\n",
       " 'for',\n",
       " 'is',\n",
       " 'job',\n",
       " 'key',\n",
       " 'learning',\n",
       " 'machine',\n",
       " 'of',\n",
       " 'science',\n",
       " 'the'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650045716403,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "xr4dB_lTAPYU"
   },
   "outputs": [],
   "source": [
    "wordDict1 = dict.fromkeys(join, 0)\n",
    "wordDict2 = dict.fromkeys(join, 0)\n",
    "\n",
    "for word in sentence1:\n",
    "  wordDict1[word] += 1\n",
    "  \n",
    "\n",
    "for word in sentence2:\n",
    "  wordDict2[word] += 1\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1650045716404,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "jwCr2ySMBd6K",
    "outputId": "6c2d04d2-5846-4f45-dffa-316b1d6bb4a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>for</th>\n",
       "      <th>century</th>\n",
       "      <th>Science</th>\n",
       "      <th>job</th>\n",
       "      <th>key</th>\n",
       "      <th>of</th>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <th>machine</th>\n",
       "      <th>21st</th>\n",
       "      <th>best</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the  is  learning  for  century  Science  job  key  of  data  science  \\\n",
       "0    2   1         0    0        1        1    1    0   1     0        0   \n",
       "1    1   1         1    1        0        0    0    1   0     1        1   \n",
       "\n",
       "   machine  21st  best  Data  \n",
       "0        0     1     1     1  \n",
       "1        1     0     0     0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDict1, wordDict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1650045716404,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "025IRkflButa"
   },
   "outputs": [],
   "source": [
    "def getTF(wordDict, data):\n",
    "  res = {}\n",
    "  corpusCount = len(data)\n",
    "  for word, count in wordDict.items():\n",
    "    res[word] = count/float(corpusCount)\n",
    "  return res\n",
    "\n",
    "tf1 = getTF(wordDict1, sentence1)\n",
    "tf2 = getTF(wordDict2, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1650045716404,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "El5XYOWjDcCy",
    "outputId": "31d6a639-7519-42f4-ed69-b26b6c93ab30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.125,\n",
       " 'is': 0.125,\n",
       " 'learning': 0.125,\n",
       " 'for': 0.125,\n",
       " 'century': 0.0,\n",
       " 'Science': 0.0,\n",
       " 'job': 0.0,\n",
       " 'key': 0.125,\n",
       " 'of': 0.0,\n",
       " 'data': 0.125,\n",
       " 'science': 0.125,\n",
       " 'machine': 0.125,\n",
       " '21st': 0.0,\n",
       " 'best': 0.0,\n",
       " 'Data': 0.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1650045716405,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "6R3Ovff0DdJC"
   },
   "outputs": [],
   "source": [
    "tf = pd.DataFrame([tf1, tf2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1650045716405,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "cd4-91qnD8IY",
    "outputId": "361049fb-1582-4f39-dc63-7f2e21825670"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>for</th>\n",
       "      <th>century</th>\n",
       "      <th>Science</th>\n",
       "      <th>job</th>\n",
       "      <th>key</th>\n",
       "      <th>of</th>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <th>machine</th>\n",
       "      <th>21st</th>\n",
       "      <th>best</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     the     is  learning    for  century  Science  job    key   of   data  \\\n",
       "0  0.200  0.100     0.000  0.000      0.1      0.1  0.1  0.000  0.1  0.000   \n",
       "1  0.125  0.125     0.125  0.125      0.0      0.0  0.0  0.125  0.0  0.125   \n",
       "\n",
       "   science  machine  21st  best  Data  \n",
       "0    0.000    0.000   0.1   0.1   0.1  \n",
       "1    0.125    0.125   0.0   0.0   0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1650045716405,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "2eNB1bkhEALq",
    "outputId": "562f674e-e166-432b-cb39-b9e318ea009c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'century',\n",
       " 'Science',\n",
       " 'job',\n",
       " 'key',\n",
       " 'data',\n",
       " 'science',\n",
       " 'machine',\n",
       " '21st',\n",
       " 'best',\n",
       " 'Data']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [w for w in wordDict1 if w not in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNIVLjWnI2zR"
   },
   "source": [
    "## Inverse Document Frequency **(IDF)**\n",
    "### Formula: idf(t) = log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1650045716406,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "ypFAb8B_Gl8d"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def getIDF(documents):\n",
    "  n = len(documents)\n",
    "  res = {}\n",
    "  res = dict.fromkeys(documents[0].keys(), 0)\n",
    "\n",
    "  for word, count in res.items():\n",
    "    res[word] = math.log10(n / (float(count) + 1))\n",
    "  return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1650045716406,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "j1laS80XKK7i",
    "outputId": "2cd9b8c6-9c01-40e5-e579-efac8f3654bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.3010299956639812,\n",
       " 'is': 0.3010299956639812,\n",
       " 'learning': 0.3010299956639812,\n",
       " 'for': 0.3010299956639812,\n",
       " 'century': 0.3010299956639812,\n",
       " 'Science': 0.3010299956639812,\n",
       " 'job': 0.3010299956639812,\n",
       " 'key': 0.3010299956639812,\n",
       " 'of': 0.3010299956639812,\n",
       " 'data': 0.3010299956639812,\n",
       " 'science': 0.3010299956639812,\n",
       " 'machine': 0.3010299956639812,\n",
       " '21st': 0.3010299956639812,\n",
       " 'best': 0.3010299956639812,\n",
       " 'Data': 0.3010299956639812}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = getIDF([wordDict1, wordDict2])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1650045716406,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "ikZir7j-KWSJ"
   },
   "outputs": [],
   "source": [
    "def getTFIDF(tf, idf):\n",
    "  tfidf = {}\n",
    "  for word, count in tf.items():\n",
    "    tfidf[word] = count*idf[word]\n",
    "  return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1650045889695,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "4Gqo2syUK3K4",
    "outputId": "a7607c70-0c28-455c-8609-7fff775f50d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>for</th>\n",
       "      <th>century</th>\n",
       "      <th>Science</th>\n",
       "      <th>job</th>\n",
       "      <th>key</th>\n",
       "      <th>of</th>\n",
       "      <th>data</th>\n",
       "      <th>science</th>\n",
       "      <th>machine</th>\n",
       "      <th>21st</th>\n",
       "      <th>best</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        the        is  learning       for   century   Science       job  \\\n",
       "0  0.060206  0.030103  0.000000  0.000000  0.030103  0.030103  0.030103   \n",
       "1  0.037629  0.037629  0.037629  0.037629  0.000000  0.000000  0.000000   \n",
       "\n",
       "        key        of      data   science   machine      21st      best  \\\n",
       "0  0.000000  0.030103  0.000000  0.000000  0.000000  0.030103  0.030103   \n",
       "1  0.037629  0.000000  0.037629  0.037629  0.037629  0.000000  0.000000   \n",
       "\n",
       "       Data  \n",
       "0  0.030103  \n",
       "1  0.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1 = getTFIDF(tf1, idfs)\n",
    "tfidf2 = getTFIDF(tf2, idfs)\n",
    "\n",
    "pdTFIDF = pd.DataFrame([tfidf1, tfidf2])\n",
    "pdTFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQEWZAnuMhs9"
   },
   "source": [
    "## TFIDF using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1650046418727,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "9wyt1pVOLfG4",
    "outputId": "9f9080c4-ac77-49b7-b4f5-710c2dea51e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Idf values:\n",
      "21st : 1.4054651081081644\n",
      "century : 1.4054651081081644\n",
      "data : 1.0\n",
      "for : 1.4054651081081644\n",
      "is : 1.0\n",
      "job : 1.4054651081081644\n",
      "key : 1.4054651081081644\n",
      "learning : 1.4054651081081644\n",
      "machine : 1.4054651081081644\n",
      "of : 1.4054651081081644\n",
      "science : 1.0\n",
      "sexiest : 1.4054651081081644\n",
      "the : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "firstV= \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV= \"machine learning is the key for data science\"\n",
    "\n",
    "vectorize= TfidfVectorizer()\n",
    "\n",
    "response= vectorize.fit_transform([firstV, secondV])\n",
    "\n",
    "# get idf values\n",
    "print('\\nIdf values:')\n",
    "for ele1, ele2 in zip(vectorize.get_feature_names(), vectorize.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1650046516786,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "W-L3q7MWMnOa",
    "outputId": "f23621de-7dbe-4774-83c2-234164e4e332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tf-Idf values:\n",
      "  (0, 1)\t0.34211869506421816\n",
      "  (0, 0)\t0.34211869506421816\n",
      "  (0, 9)\t0.34211869506421816\n",
      "  (0, 5)\t0.34211869506421816\n",
      "  (0, 11)\t0.34211869506421816\n",
      "  (0, 12)\t0.48684053853849035\n",
      "  (0, 4)\t0.24342026926924518\n",
      "  (0, 10)\t0.24342026926924518\n",
      "  (0, 2)\t0.24342026926924518\n",
      "  (1, 3)\t0.40740123733358447\n",
      "  (1, 6)\t0.40740123733358447\n",
      "  (1, 7)\t0.40740123733358447\n",
      "  (1, 8)\t0.40740123733358447\n",
      "  (1, 12)\t0.28986933576883284\n",
      "  (1, 4)\t0.28986933576883284\n",
      "  (1, 10)\t0.28986933576883284\n",
      "  (1, 2)\t0.28986933576883284\n"
     ]
    }
   ],
   "source": [
    "print('\\nTf-Idf values:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 534,
     "status": "ok",
     "timestamp": 1650046790805,
     "user": {
      "displayName": "Onasvee Banarse",
      "userId": "05183216069946184409"
     },
     "user_tz": -330
    },
    "id": "iCYJ4b-gM9MZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practical7edit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
